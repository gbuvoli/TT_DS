{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6cd5b1",
   "metadata": {},
   "source": [
    "# Clase 1: Introducción al Procesamiento de Lenguaje Natural (NLP)\n",
    "\n",
    "## 1. ¿Qué es el NLP?\n",
    "\n",
    "El Procesamiento de Lenguaje Natural (NLP) es una rama de la Inteligencia Artificial que permite que las máquinas entiendan, procesen y generen lenguaje humano.\n",
    "\n",
    "El objetivo principal es transformar texto (no estructurado) en datos numéricos que los modelos puedan analizar.\n",
    "\n",
    "---\n",
    "\n",
    "![usosNLP](https://f5b623aa.delivery.rocketcdn.me/wp-content/uploads/2022/10/New-Use-Cases-760px.jpg.avif)\n",
    "\n",
    "\n",
    "## 2. ¿Por qué el texto es un problema para Machine Learning?\n",
    "\n",
    "Los modelos de Machine Learning requieren **números**, pero el texto es:\n",
    "\n",
    "- Ambiguo  \n",
    "- Desordenado  \n",
    "- Inconsistente  \n",
    "- Extenso  \n",
    "\n",
    "Por eso el NLP utiliza técnicas para **convertir texto → números**, como:\n",
    "\n",
    "- Tokenización  \n",
    "- Normalización  \n",
    "- Limpieza  \n",
    "- Vectorización  \n",
    "- Embeddings  \n",
    "\n",
    "![alt text](https://f5b623aa.delivery.rocketcdn.me/wp-content/uploads/2022/10/How-NLP-Works-760px.jpg.avif)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Machine Learning para Textos: visión general\n",
    "\n",
    "En NLP se usan dos grandes familias de modelos:\n",
    "\n",
    "### 3.1. Modelos clásicos (ML tradicional)\n",
    "\n",
    "Trabajan con representaciones simples del texto, como Bag-of-Words o TF–IDF.\n",
    "\n",
    "Ejemplos:\n",
    "- Regresión Logística  \n",
    "- Naive Bayes  \n",
    "- SVM  \n",
    "- KNN  \n",
    "- Árboles y Random Forest  \n",
    "\n",
    "Aplicaciones:\n",
    "- Clasificación de sentimientos  \n",
    "- Spam detection  \n",
    "- Clasificación de documentos  \n",
    "\n",
    "### 3.2. Modelos modernos (Deep Learning)\n",
    "\n",
    "Aprenden representaciones profundas del lenguaje.\n",
    "\n",
    "Ejemplos:\n",
    "- Embeddings (Word2Vec, GloVe, FastText)  \n",
    "- RNN, LSTM, GRU  \n",
    "- Modelos Transformer (BERT, GPT, T5, RoBERTa)  \n",
    "\n",
    "![challenges ](https://f5b623aa.delivery.rocketcdn.me/wp-content/uploads/2022/10/Challenges-760px.jpg.avif)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Flujo de trabajo típico en NLP\n",
    "\n",
    "![alt text](https://datos.gob.es/sites/default/files/datosgobes/esquema_pasos.png)\n",
    "\n",
    "\n",
    "1. Recolección del texto  \n",
    "2. Limpieza y normalización  \n",
    "3. Tokenización  \n",
    "4. Vectorización  \n",
    "5. Entrenamiento del modelo  \n",
    "6. Predicción  \n",
    "7. Evaluación  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7934f4",
   "metadata": {},
   "source": [
    "## Flujo de trabajo en NLP: Métodos, enfoques y librerías\n",
    "\n",
    "En esta sección desglosamos cada paso del flujo de trabajo típico en NLP, explicando **qué se hace, por qué se hace** y **cuáles son los métodos más comunes**, sin incluir código.  \n",
    "Será el “mapa mental” para que tus estudiantes entiendan el panorama completo del procesamiento de texto.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Recolección del Texto\n",
    "\n",
    "La calidad del dataset determina la calidad del modelo.  \n",
    "Fuentes típicas:\n",
    "\n",
    "- Comentarios de usuarios  \n",
    "- Reseñas de productos  \n",
    "- Chats, emails  \n",
    "- Noticias, blogs, redes sociales  \n",
    "- Archivos PDF, TXT, CSV  \n",
    "- APIs (Twitter, Reddit, HuggingFace Datasets)\n",
    "\n",
    "**Consideraciones importantes:**\n",
    "- Tamaño del corpus  \n",
    "- Dominio del lenguaje (médico, legal, redes sociales)  \n",
    "- Ruido (errores ortográficos, emojis, siglas)  \n",
    "- Etiquetas disponibles (clasificación, sentimiento, intención)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Limpieza y Normalización del Texto\n",
    "\n",
    "Transformar el texto crudo en algo más homogéneo y analizable.\n",
    "\n",
    "#### Técnicas comunes\n",
    "\n",
    "##### **2.1 Lowercasing**\n",
    "Unifica representación:\n",
    "- “Película” = “película” = “PELÍCULA”\n",
    "\n",
    "##### **2.2 Normalización Unicode**\n",
    "Corrige caracteres extraños provenientes de HTML, PDF, acentos “compuestos”.\n",
    "\n",
    "##### **2.3 Eliminación de ruido**\n",
    "Dependiendo del proyecto:\n",
    "- URLs  \n",
    "- Números  \n",
    "- Emojis  \n",
    "- Signos repetidos  \n",
    "- HTML tags  \n",
    "- Menciones `@usuario`  \n",
    "\n",
    "##### **2.4 Stopwords**\n",
    "Palabras muy frecuentes y poco informativas: “el”, “y”, “pero”, “de”.\n",
    "\n",
    "##### **2.5 Lematización**\n",
    "Convierte palabras en su forma base:\n",
    "- “Caminando”, “caminará”, “caminaron” → **caminar**\n",
    "\n",
    "Motor común: **spaCy**, **Stanza**.\n",
    "\n",
    "##### **2.6 Stemming**\n",
    "Recorta palabras a su raíz morfológica:\n",
    "- “Caminando”, “caminar”, “caminaron” → **camin**\n",
    "\n",
    "Más agresivo. Librería: **NLTK**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Tokenización\n",
    "\n",
    "Convertir texto en unidades mínimas.\n",
    "\n",
    "##### **3.1 Tokenización por palabras**\n",
    "Divide el texto en palabras según espacios y reglas del lenguaje.\n",
    "\n",
    "Librerías:\n",
    "- NLTK\n",
    "- spaCy\n",
    "- HuggingFace Tokenizers\n",
    "\n",
    "##### **3.2 Tokenización por frases**\n",
    "Útil en análisis de documentos largos.\n",
    "\n",
    "##### **3.3 Tokenización basada en subpalabras**\n",
    "Usada en modelos modernos:\n",
    "- **Byte-Pair Encoding (BPE)**\n",
    "- **WordPiece (BERT, DistilBERT)**\n",
    "- **SentencePiece (T5, ALBERT)**\n",
    "\n",
    "Soluciona:\n",
    "- el problema del vocabulario infinito  \n",
    "- palabras desconocidas  \n",
    "- manejo eficiente de morfología\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Representación del Texto (Vectorización)\n",
    "\n",
    "Convertir texto → números.  \n",
    "Aquí hay 2 mundos: **modelos clásicos** y **modelos profundos**.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4.1 Bag of Words (BoW)\n",
    "\n",
    "Representa el texto como una bolsa de palabras, sin orden ni contexto.\n",
    "\n",
    "![alt text](https://old.tacosdedatos.com/assets/detrasdelavis/004_frequency.png)\n",
    "\n",
    "**Ventajas:**\n",
    "- Simple  \n",
    "- Rápido  \n",
    "- Funciona bien en clasificación básica  \n",
    "\n",
    "**Limitaciones:**\n",
    "- No captura contexto  \n",
    "- No entiende orden  \n",
    "- Matrices enormes y dispersas  \n",
    "\n",
    "---\n",
    "\n",
    "##### 4.2 N-gramas\n",
    "\n",
    "Una extensión de BoW que almacena secuencias de palabras.\n",
    "\n",
    "Ejemplos:\n",
    "- unigramas: “buena”, “película”\n",
    "- bigramas: “buena película”\n",
    "- trigramas: “no me gustó”\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/word2vec/lm-sliding-window-4.png)\n",
    "\n",
    "Ayuda en:\n",
    "- detección de expresiones comunes  \n",
    "- negaciones (“no bueno”)  \n",
    "- contexto local  \n",
    "\n",
    "[More info +](https://old.tacosdedatos.com/word-to-vec-ilustrado)\n",
    "---\n",
    "\n",
    "##### 4.3 TF–IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "Pondera palabras según su importancia.\n",
    "\n",
    "Idea:\n",
    "- palabras frecuentes en un documento → importantes  \n",
    "- palabras muy frecuentes en todos los documentos → menos importantes  \n",
    "\n",
    "![alt text](https://old.tacosdedatos.com/assets/detrasdelavis/004_tfidf_no_norm.png)\n",
    "\n",
    "**Ventajas:**\n",
    "- buena base para modelos clásicos  \n",
    "- reduce ruido  \n",
    "- mantiene interpretación  \n",
    "\n",
    "**Desventajas:**\n",
    "- aún ignora el contexto semántico  \n",
    "- sensible a vocabularios grandes  \n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Embeddings (Representaciones Densas)\n",
    "\n",
    "Representan palabras como **vectores de números reales**, donde palabras con significados similares tienen vectores similares.\n",
    "\n",
    "\n",
    "![alt text](https://web.engr.oregonstate.edu/~huanlian/teaching/ML/2024fall/unit4/figs/word_embeddings_visualization.png)\n",
    "\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/word2vec/king-man-woman-embedding.png)\n",
    "\n",
    "![alt text](https://jalammar.github.io/images/word2vec/queen-woman-girl-embeddings.png)\n",
    "\n",
    "#### ¿Qué resuelven?\n",
    "- Semántica (“rey” parecido a “reina”)  \n",
    "- Relaciones (“parís” - “francia” ≈ “berlín” - “alemania”)  \n",
    "- Problema del vocabulario grande  \n",
    "\n",
    "#### Enfoques principales de embeddings:\n",
    "\n",
    "---\n",
    "\n",
    "##### **A. Word2Vec** (Google)\n",
    "\n",
    "Modelos:\n",
    "- **CBOW** (predice palabra a partir de contexto)\n",
    "- **Skip-gram** (predice contexto a partir de una palabra)\n",
    "\n",
    "Aprenden relaciones semánticas sorprendentes.\n",
    "\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/word2vec/language_model_blackbox_output_vector.png)\n",
    "\n",
    "---\n",
    "\n",
    "##### **B. GloVe** (Stanford)\n",
    "\n",
    "Basado en factorización de matrices y coocurrencias.  \n",
    "Aprende relaciones globales del corpus.\n",
    "\n",
    "---\n",
    "\n",
    "##### **C. FastText** (Facebook)\n",
    "\n",
    "Incluye subpalabras:\n",
    "- “correr”, “corriendo”, “corrió” → vectores relacionados\n",
    "\n",
    "Excelente para lenguajes flexivos (como español).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 Embeddings contextuales (Transformers)\n",
    "\n",
    "Revolucionaron el NLP porque entienden contexto **palabra por palabra** dentro de la frase.\n",
    "\n",
    "#### **Modelos principales**\n",
    "\n",
    "##### **BERT** (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT es uno de los modelos más importantes en la historia del NLP.  \n",
    "Fue desarrollado por Google en 2018 y **cambió por completo** la forma en que las máquinas entienden lenguaje.\n",
    "\n",
    "A diferencia de los modelos clásicos o las RNN/LSTM, BERT **entiende el texto en contexto**, leyendo en ambas direcciones al mismo tiempo.\n",
    "\n",
    "Antes de BERT:\n",
    "\n",
    "- Los modelos leían **de izquierda a derecha** (como un texto normal)  \n",
    "- O de derecha a izquierda  \n",
    "- O en un solo sentido (RNN, LSTM, GRU)  \n",
    "- O con contexto local limitado (CNN, n-gramas)\n",
    "\n",
    "BERT rompe esta limitación:\n",
    "> BERT lee el texto **simultáneamente hacia adelante y hacia atrás**, entendiendo el contexto completo.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "En la frase:\n",
    "> “El banco está cerrado”\n",
    "\n",
    "BERT capta que “banco” significa **institución financiera**, no un asiento.  \n",
    "Por eso se dice que entiende *contexto profundo*.\n",
    "\n",
    "Características clave de BERT:\n",
    "\n",
    "- Lee la frase en ambas direcciones  \n",
    "- Capta contexto profundo  \n",
    "- Base de miles de variantes  \n",
    "\n",
    "##### **RoBERTa, ALBERT, DistilBERT**\n",
    "Versiones optimizadas o más ligeras.\n",
    "\n",
    "##### **GPT / T5 / LLaMA / Mistral**\n",
    "Modelos generativos autoregresivos.\n",
    "\n",
    "\n",
    "[Aprendamos un poco mas sobre BERT y su impacto.](https://youtu.be/MdEYUliufmk?si=N4SihF6iP09jHocU)\n",
    "---\n",
    "\n",
    "### ¿Por qué los embeddings contextuales son superiores?\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "- “banco” (sentarse)  \n",
    "- “banco” (institución financiera)\n",
    "\n",
    "BoW y TF-IDF los tratarían igual.  \n",
    "Un Transformer **no**, porque interpreta la frase completa.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Entrenamiento del Modelo\n",
    "\n",
    "Dependiendo de la representación del texto, se eligen distintos modelos.\n",
    "\n",
    "### **Modelos clásicos**\n",
    "Adecuados para BoW y TF-IDF:\n",
    "\n",
    "- Regresión Logística  \n",
    "- Naive Bayes  \n",
    "- SVM  \n",
    "- Random Forest  \n",
    "- KNN  \n",
    "\n",
    "**Pros:** rápidos, interpretables  \n",
    "**Contras:** no captan semántica profunda  \n",
    "\n",
    "### **Modelos basados en embeddings**\n",
    "- Redes Neuronales densas  \n",
    "- CNN para texto  \n",
    "- RNN / LSTM / GRU  \n",
    "\n",
    "### **Transformers**\n",
    "- Fine-tuning de BERT  \n",
    "- Modelos generativos (GPT-like)  \n",
    "- Clasificación, QA, NER, resúmenes\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Predicción\n",
    "\n",
    "Una vez entrenado el modelo:\n",
    "- se preprocesa el texto nuevo  \n",
    "- se vectoriza con las mismas reglas  \n",
    "- se hace la predicción  \n",
    "\n",
    "Tipos de tareas:\n",
    "- clasificación (sentimiento, intención)  \n",
    "- regresión (puntajes de opinión)  \n",
    "- etiquetado (NER)  \n",
    "- generación (traducción, resumen)  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Evaluación\n",
    "\n",
    "Métricas comunes:\n",
    "\n",
    "### **7.1 Clasificación**\n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F1-score  \n",
    "- Matriz de confusión  \n",
    "\n",
    "### **7.2 Modelos de lenguaje**\n",
    "- Perplexity  \n",
    "- BLEU score (traducción)  \n",
    "- ROUGE (resumen)  \n",
    "\n",
    "### **7.3 Evaluación humana**\n",
    "Crucial en sistemas generativos:\n",
    "- coherencia  \n",
    "- fluidez  \n",
    "- relevancia  \n",
    "\n",
    "---\n",
    "\n",
    "# Resumen visual del ecosistema NLP\n",
    "\n",
    "- **BoW / N-gramas** → modelos clásicos  \n",
    "- **TF-IDF** → potente y simple  \n",
    "- **Embeddings estáticos** (Word2Vec, GloVe, FastText) → relaciones semánticas  \n",
    "- **Embeddings contextuales** (BERT, GPT) → contexto profundo y comprensión real del lenguaje  \n",
    "\n",
    "Este desglose prepara el terreno para la implementación práctica en la siguiente clase.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
